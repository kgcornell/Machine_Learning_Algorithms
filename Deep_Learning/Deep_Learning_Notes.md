# Deep Learning - Theory
Most deep learning algorithms are based on an optimization algorithm called stochastic gradient descent.

In the domain of unsupervised learning, the aim of deep learning models is to learn the entire probability distribution that generated the data set in the first place. This is useful for tasks involving density estimation or implicitly for tasks like synthesis or denoising.

Methods of quantifying model capacity such as **Vapnik - Chervonenkis** dimension generally do not work very well for deep learning algorithms. This is partly because the bounds are loose and it can also be very difficult to determine the capacity of deep learning algorithms.
