{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content below has been picked from [TensorFlow Organization](https://www.tensorflow.org/versions/r1.1/get_started/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Core programs consist of two discrete sections:\n",
    "* Building the computational graph\n",
    "* Running the computational graph\n",
    "\n",
    "A computational graph is a series of TensorFlow operations arranged into a graph of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all Tensorflow constants, it takes no inputs, and it outputs a value it stores internally. We create two floating point tensors node1 and node2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "print(node1,node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not get the values as we would expect. Instead we get nodes which when evaluated would produce the values respectively. For this we need to run the graph in a **session**. A session encapsulates the state and control of the TensorFlow runtime.\n",
    "\n",
    "The following code creates a session object and then invokes its run method to run enough of the computational graph to evaluate node1 and node2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1,node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicated computations can be performed by combining Tensor nodes with operations. Operations are also nodes. For example we can add two constant nodes and produce a new graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node3): \",sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlow provides a utility board called TensorBoard that can display a picture of the computational graph. A graph can also be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to add a value later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict parameter to specify Tensors that provide concrete values to these placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can make the computational graph more complex by adding another operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node*3.\n",
    "print(sess.run(add_and_triple,{a:3,b:4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. **Variables** allow us to add trainable parameters to a graph. They are constructed with a type and initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3],tf.float32)\n",
    "b = tf.Variable([-.3],tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized when we call tf.constant, and their value can never change. By contrast, variables are not initialized when we call tf.variable. To initialize all the variables in a Tensorflow program, you must explicitly call a special operation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that init is a handle to the TensorFlow sub-graph that initialzes all the global variables. Until we call sess.run, the variables are uninitialized.\n",
    "\n",
    "Since x is placeholder, we can evaluate linear_model for several values of x simultaneously as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model,{x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a model, but we do not know how good it is yet. To evaluate the model on training data, we need a y placeholder to provide the desired values, and we need to write a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function measures how far apart the current model is from provided data. We will use the standard loss model for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this manually by reassigning the values of W and b to the perfect values of -1 and 1. A variable is initialized to the value provided to tf.Variable but can be changed by using operations like tf.assign. For example, W=-1 and b=1 are the optimal parameters for our model. We can change W and b accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W,[-1.])\n",
    "fixb = tf.assign(b,[1.])\n",
    "sess.run([fixW,fixb])\n",
    "print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well here we guessed the correct values of both W and b to obtain perfect results. But in machine learning, we need to automate this process of finding the optimal value of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides **optimizers** that slowly change each variable in order to minimize the loss function. The simplest optimizer is **gradient descent**. TensorFlow can automatically produce derivatives given only a description of the model using the function **tf.gradients**. For simplicity, optimizers does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init) #reset the value to incorrect defaults\n",
    "for i in range(1000):\n",
    "    sess.run(train,{x:x_train,y:y_train})\n",
    "    \n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# Evaluating training accuracy\n",
    "curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code amounts to a very simple machine learning model. TensorFlow provides higher levels of abstractions for common patterns, structures and functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib.learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.contrib.learn is a high-level TensorFlow Library that simplifies the mechanics of machine learning. The basic usage is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare a list of features. We only have one real valued feature in this case. There are many other types of column.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\",dimension=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator is the front end to invoke training (fitting) and evaluation (inference). The following code provides an estimator that does linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C20132D240>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\karti\\AppData\\Local\\Temp\\tmpyysxsp1r\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides many helper methods to read and set up data sets. Here we use 'numpy_input_fn'. We have to tell the function how many batches of data (num_epochs) we want and how big each batch should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1.,2.,3.,4.])\n",
    "y = np.array([0.,-1.,-2.,-3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke 1000 training steps by invoking the 'fit' method and passing the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From C:\\Users\\karti\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\karti\\AppData\\Local\\Temp\\tmpyysxsp1r\\model.ckpt.\n",
      "INFO:tensorflow:loss = 5.75, step = 1\n",
      "INFO:tensorflow:global_step/sec: 765.394\n",
      "INFO:tensorflow:loss = 0.0457657, step = 101 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.473\n",
      "INFO:tensorflow:loss = 0.00469578, step = 201 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 750.492\n",
      "INFO:tensorflow:loss = 0.00964848, step = 301 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.831\n",
      "INFO:tensorflow:loss = 0.0015457, step = 401 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 723.175\n",
      "INFO:tensorflow:loss = 0.000245794, step = 501 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 749.826\n",
      "INFO:tensorflow:loss = 0.000208762, step = 601 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 749.994\n",
      "INFO:tensorflow:loss = 2.38684e-05, step = 701 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 750.857\n",
      "INFO:tensorflow:loss = 1.62499e-05, step = 801 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 999.929\n",
      "INFO:tensorflow:loss = 1.66318e-06, step = 901 (0.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\karti\\AppData\\Local\\Temp\\tmpyysxsp1r\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.97447e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressor(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x000001C201133780>, 'feature_columns': [_RealValuedColumn(column_name='x', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)], 'optimizer': None, 'gradient_clip_norm': None, 'joint_weights': False})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(input_fn=input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From C:\\Users\\karti\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-14-16:09:41\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\karti\\AppData\\Local\\Temp\\tmpyysxsp1r\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-14-16:09:43\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 8.78245e-07\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'loss': 8.7824532e-07, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Here we evaluate how well our model did. In a real example, we would want\n",
    "# to use a separate validation and testing data set to avoid overfitting.\n",
    "print(estimator.evaluate(input_fn=input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Developing a Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to combine the high level abstraction of data set, feeding, training etc of tf.contrib.learn and define our own custom models. For this we need to use *tf.contrib.learn.Estimator* and provide it with our own function. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C2047E4EF0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\karti\\AppData\\Local\\Temp\\tmpkanzyvil\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\karti\\AppData\\Local\\Temp\\tmpkanzyvil\\model.ckpt.\n",
      "INFO:tensorflow:loss = 30.6799646568, step = 1\n",
      "INFO:tensorflow:global_step/sec: 748.769\n",
      "INFO:tensorflow:loss = 0.0130359686035, step = 101 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 754.462\n",
      "INFO:tensorflow:loss = 0.00203925428829, step = 201 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 745.149\n",
      "INFO:tensorflow:loss = 0.000115631565643, step = 301 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 754.66\n",
      "INFO:tensorflow:loss = 1.43864484855e-05, step = 401 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.353\n",
      "INFO:tensorflow:loss = 1.33497202029e-06, step = 501 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.254\n",
      "INFO:tensorflow:loss = 1.599542311e-07, step = 601 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 654.849\n",
      "INFO:tensorflow:loss = 9.85512610261e-09, step = 701 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.092\n",
      "INFO:tensorflow:loss = 4.97197783347e-10, step = 801 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 850.891\n",
      "INFO:tensorflow:loss = 9.27409810518e-11, step = 901 (0.121 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\karti\\AppData\\Local\\Temp\\tmpkanzyvil\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.78033501039e-12.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-14-16:34:16\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\karti\\AppData\\Local\\Temp\\tmpkanzyvil\\model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-14-16:34:17\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 3.1165e-12\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'loss': 3.1164984e-12, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Declare a list of features. In this case, we only have one feature.\n",
    "def model(features, labels, mode):\n",
    "    # Build a linear model and predict values\n",
    "    W = tf.get_variable(\"W\",[1],dtype=tf.float64)\n",
    "    b = tf.get_variable(\"b\",[1],dtype=tf.float64)\n",
    "    y = W*features['x']+b\n",
    "    # Loss sub-graph\n",
    "    loss = tf.reduce_sum(tf.square(y-labels))\n",
    "    # Training sub-graph\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),tf.assign_add(global_step,1))\n",
    "    # ModelFnOps connects subgraphs we built to the appropriate functionality\n",
    "    return tf.contrib.learn.ModelFnOps(mode=mode, predictions=y,loss=loss,train_op=train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# evaluate our model\n",
    "print(estimator.evaluate(input_fn=input_fn, steps=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the contents of the custom model() function are very similar to our manual model training loop from the lower level API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
